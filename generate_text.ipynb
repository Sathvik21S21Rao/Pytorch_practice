{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sathvik21S21Rao/Pytorch_practice/blob/main/generate_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ahPITEpSnsEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk2Ia2VNntPL",
        "outputId": "ee1f2946-1637-4edd-b073-2a4c51aa760e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBAq0c6ZIhml",
        "outputId": "cfcfbb72-98f7-418b-dd84-18e3bff396ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model\n",
        "Using tiktoken(gpt-2) tokenizing model to tokenize the text file\n",
        "Using pytorch to train the model to learn to generate the next token based on the previous token."
      ],
      "metadata": {
        "id": "KQ3NBk2O9uVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy"
      ],
      "metadata": {
        "id": "NROev3eEn1zx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "hIDVqj2NhBrA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc=tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "jzZ5QgH2n4gm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector=enc.encode(open(\"Hamming.txt\").read().lower().replace(\"\\n\",\"\"))"
      ],
      "metadata": {
        "id": "YP_VZNotou27"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3ABW3UDCEYF",
        "outputId": "5f0eeaee-bf62-4f56-a37a-2f7191ad86bd"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "508"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def generate_batch(batch_size,vector_size):\n",
        "  index=[random.randrange(0,len(vector)-vector_size-2) for i in range(batch_size)]\n",
        "  input_vectors=torch.tensor([vector[index[j]:index[j]+vector_size] for j in range(batch_size)]).to(device)\n",
        "  target_vectors=torch.tensor([vector[index[j]+1:index[j]+vector_size+1] for j in range(batch_size)]).to(device)\n",
        "  return input_vectors,target_vectors"
      ],
      "metadata": {
        "id": "ks5jRfEAAu3J"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query = nn.Linear(in_dim, out_dim)\n",
        "        self.key = nn.Linear(in_dim, out_dim)\n",
        "        self.value = nn.Linear(in_dim, out_dim)\n",
        "        self.out_dim=out_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        attention_scores = torch.matmul(q, k.transpose(1,2))/(self.out_dim)**0.5\n",
        "        B,T,C=attention_scores.shape\n",
        "        mask=torch.tril(torch.ones(T,T)).to(device)\n",
        "        if mask is not None:\n",
        "          attention_scores=attention_scores.masked_fill(mask==0,float(\"-inf\"))\n",
        "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
        "        output = torch.matmul(attention_weights, v)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zNwX1TBxAz7x"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenModel(nn.Module):\n",
        "  def __init__(self,out_features:int,hidden:int):\n",
        "    super().__init__()\n",
        "    self.embedding=nn.Embedding(enc.n_vocab+1,hidden)\n",
        "    self.self_attention=SelfAttention(hidden,hidden)\n",
        "    self.layer=nn.Sequential(nn.Linear(in_features=hidden,out_features=hidden,bias=True),\n",
        "                             nn.GELU(),\n",
        "                             nn.Linear(in_features=hidden,out_features=enc.n_vocab,bias=True),\n",
        "                             )\n",
        "  def forward(self,x):\n",
        "    return self.layer(self.self_attention(self.embedding(x)))\n"
      ],
      "metadata": {
        "id": "yNXVrtiLuss8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=TokenModel(500,500).to(device)"
      ],
      "metadata": {
        "id": "SOWbJHH66ljq"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(params=model.parameters(),lr=0.001,)\n"
      ],
      "metadata": {
        "id": "R0XyaBDP7RH0"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs:int,model,loss_fn,optimizer):\n",
        "  for i in range(epochs):\n",
        "    model.train()\n",
        "    x,y=generate_batch(4,10)\n",
        "    y_logit=model(x)\n",
        "    B,T,C=y_logit.shape\n",
        "    y_logit=y_logit.view(B*T,C)\n",
        "    y=y.view(B*T)\n",
        "    loss=loss_fn(y_logit,y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "eXELE3sOIUQe"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(10000,model,loss_fn,optimizer)\n",
        "g=torch.Generator(device=\"cuda\")\n",
        "g.manual_seed(2147483647)"
      ],
      "metadata": {
        "id": "aR4DDeZsKjk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da5803e-087f-4aca-ce2c-187e22bbe64c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7df1d88a2d50>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model,tokens):\n",
        "  x=generate_batch(4,10)[0]\n",
        "  result=copy.deepcopy(x).to(device)\n",
        "  for i in range(tokens):\n",
        "   y_logit=model(x)\n",
        "   y_logit=y_logit[:,-1,:].to(device)\n",
        "   y_pred=torch.softmax(y_logit,dim=1).to(device)\n",
        "   ans=torch.multinomial(y_pred,num_samples=1,generator=g).to(device)\n",
        "   result=torch.cat([result,ans],dim=1).to(device)\n",
        "   x=torch.cat([x,ans],dim=1).to(device)\n",
        "   x=x[:,1:]\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "sxLjdYZ-Nrlb"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p '/content/drive/MyDrive/Models'"
      ],
      "metadata": {
        "id": "V8gQ_VcGQCM4"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Loss"
      ],
      "metadata": {
        "id": "EIrfeKz9K2MC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg=0\n",
        "with torch.inference_mode():\n",
        "  for i in range(100):\n",
        "    loss_batch=generate_batch(4,8)\n",
        "    y_logit=model(loss_batch[0])\n",
        "    B,T,C=y_logit.shape\n",
        "    y_logit=y_logit.view(B*T,C)\n",
        "    y=loss_batch[1].view(B*T)\n",
        "    avg+=nn.functional.cross_entropy(y_logit,y).item()\n",
        "  print(avg/100)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H4L_c_TQ5wA",
        "outputId": "f8a7c3e4-9a98-45ef-a39c-e104c4a07b08"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2006141656637193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),\"/content/drive/MyDrive/Models/model.pth\")"
      ],
      "metadata": {
        "id": "CoL3CtEox9Ib"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the torch model"
      ],
      "metadata": {
        "id": "ScgARxog-Si5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=TokenModel(500,500)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Models/model.pth\"))\n",
        "model=model.to(device)"
      ],
      "metadata": {
        "id": "RSzyw2X2zeUp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model,tokens):\n",
        "  x=generate_batch(4,10)[0]\n",
        "  result=copy.deepcopy(x).to(device)\n",
        "  for i in range(tokens):\n",
        "   y_logit=model(x)\n",
        "   y_logit=y_logit[:,-1,:].to(device)\n",
        "   y_pred=torch.softmax(y_logit,dim=1).to(device)\n",
        "   ans=torch.multinomial(y_pred,num_samples=1,generator=g).to(device)\n",
        "   result=torch.cat([result,ans],dim=1).to(device)\n",
        "   x=torch.cat([x,ans],dim=1).to(device)\n",
        "   x=x[:,1:]\n",
        "  return result"
      ],
      "metadata": {
        "id": "OYK1DHDCJukG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "g=torch.Generator(device=\"cuda\")\n",
        "g.manual_seed(2147483647)\n",
        "with torch.inference_mode():\n",
        "  y=generate(model,50)\n",
        "  for j in y:\n",
        "    print(enc.decode(j.tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XI1vP8cJNW3",
        "outputId": "48b3506d-7f1a-4802-b104-a00c8b2b6dfd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " for errors and correct them if necessary. if a vital component in data integrity in the of single-correcting codes are various typesstate drives and efficient solution to the parity will pinpoint be flipped implementation realm early early parity checks information is carefully to suit to be chosen chosen chosen calculated chosen to cosmic rays or\n",
            " of linear error-correcting code, specifically designed to the original information but also designed to create suit applications design and cache memory, hamming(7, allowing it to check for single-fi. in the problem realm erroneous bitbit error correction of error correction of error occurs during transmission, and information\n",
            ", where data integrity is paramount.in summary, every 4 bits to prevent and correct them if solid bits if correct them if hamming(15, is codes. this code, every 4 bits to create cosmic makes where data are used in the of hamming codes can detect and correction ofming codes\n",
            " error correction and double-bit error detection. this makes hamming codes are various types of error occurs in data to the detection and communication standards systems. they a codeword that carries not only the 1950eword. additionally, making themord that may occur due to the of error correction, such\n"
          ]
        }
      ]
    }
  ]
}